Chapter 36: I/o
CPU is attached to the main memory of the system of the system via
    some kind of memory bus or interconnect. Some of the devices are 
    to the system via a general I/O bus, which in modern systems are 
    call PCI graphics and some other high preformance I/O devices might 
    be found there 
Lower down are one or more of peripheral bus, such as SCSI, Sata, or USB
    USB devices include disks, mouse, or keyboard these are slow 

We need heirarchical structures simply for physics and cost. The faster a
    bus is the shorter is must be, thus a high performance memory bus doe
    not have much room to plug devices into 

Engineering a bus for high performance is costly so a hierarchical 
    approach where components that demand high performance (liek 
    graphics) cards are closer to the CPU. lower performance are 
    farther away 

*** A Canonical Device ***
A device has two important components. The first is hardware interface 
    and the second is internal structure.
    Hardware Interface: hardware must also have some kind of interface 
        that allows the system software to control its operation. The 
        hardware must have some kind of interface that allows the system 
        software to control its operation. Thus all devices have a 
        specified interface and protocol for typical interaction
    Internal Structure: This part of the device is implementation 
        specific and is responsible for implemenating the abstraction 
        the device presents to the system. Very simple devices will have 
        one or a few hardware chips to implement their functionality; 
        more complex devices will include a simple CPU, some general 
        purpose memory, and other device-specific chips to get their job 
        done.

*** Canonical protocol ***
Device interface is compromised of three registers:
    Status register: can be read to see the current status of the device 
    Command Register: To tell the device to perform a certain task 
    Data Register: pass data to the device or get data from the device 
Polling: OS waits until the device is ready to recieve a command by 
    repeatedly reading the status register (OS is asking whats going 
    on)
Protocols have 4 steps:
Typical interaction that the OS might have with a device is: 
    first, OS waits until the device is ready to recieve command
    second, OS sends data down to the register
    third, OS writes a command to the command register, doing so lets 
        the device know data is present and it can start working on the 
        command
    fourth, OS waits for the device to finish by again polling in a loop
        waiting to see if it is finished (it may then get an error code)


Chapter 37: HardDisk Drives
Main form of persistent data storage 

*** Interface ***
Drive consists of a large number of sectors (512-byte blocks) each of 
    which can be written or read 

The sectors are numbered from 0 to n -1 on a disk with n sectors. We can
    view the disk as an array of sectors, 0 to n-1 is thus the address
    space 

The only guarentee drive manufacturers make is that a single 512-byte 
    write is atomic, it will either complete in its entireety or it 
    wont comeplete at all, thus if a power loss occurs only a portion of
    a larger write may complete (torn write)

THere is an "unwritten contract" of disk drives. It assumes that 
    accessing two blocks near one another is faster than accessing block
    that are farther away. Thus accessing blocks in a contiguous
    (touching) chunks is the fastests access mode and usually much 
    faster than a random access pattern

*** Basic Geometry ***
Platter: a circular surface on which data is stores persistently by
    inducing magnetic changes to it. Drives may have one or more platters
    Each platter has 2 sides, each side called a surface
Spindle: The platters are bound together by the spindle, its connected to
    the motor that spins the platter around while the drive is powered on
    and it spins at a constant fixed rate. (7200 to 15000 RPM). Single 
    rotation takes around 6ms
Track: Data is encoded on each surface in concentric circles of sectors,
    we call it a track. a surface holds thousands of tracks
Disk Head/Disk Arm: The head is the hardhard that read/write onto the 
    surface. The head is moved around using the disk arm 

*** Simple Disk Drive ***
simple disk with a single track (Figure 37.1). This track has just 12 
    sectors, each of which is 512 bytes in size (our typical sector 
    size, recall) and addressed therefore by the numbers 0 through 11. 
    The single platter we have here rotates around the spindle, to which 
    a motor is attached.

*** SIngle track latency: The rotational Delay ***
To process a request hour disk just waits for the desired sector to 
    rotate under the disk head.
Rotational Delay: THe process of waiting for a sector to rotate under 
    the head. The delay can be described by (R/2)

*** Multiple tracks: seek time ***
Seek: To service a read the drive has to first move the disk arm to the
    correct track, this process is called seek
Seek has multiple phases: 
    Acceleration: the disk arm gets moving, then 
    coasting: as the arm is moving at full speed 
    deceleration: arm slows down 
    setting: the head is carefully positioned over the correct track 

After the seek, the disk arm has positioned the head over the right
    track.

The settling time is often quite significant .5 to .2 ms 

After the desired sector is under the disk head the final phase of I/O 
    will take place called transfer
    transfer: data is either read from or written to the surface

*** Other details ***
Track Skew: to make sure that sequential reads can be properly serviced 
    even when crossing track boundarie. 

Without the skew the head would be moved to the next track but the 
    desired next block would have passed thus needing to do a full 
    rotation

multi-zoned disk drives, where the disk is organized into multiple 
    zones, and where a zone is consecutive set of tracks on a surface.

cache, for historical reasons sometimes called a track buffer. This
    cache is just some small amount of memory (usually around 8 or 16 
    MB) which the drive can use to hold data read from or written to the 
    disk. 

When should the disk acknowledge the write has completed when it puts the
    data in its memory or after the write has been written to disk. 
    Write back(immidiate reporting): Complete when data is in memory 
    Write through: Complete when data is written to disk 

I/O time: Doing math for disk performance (pg 7)
T(I/O) = T(seek) + T(rotation) + T(transfer)
Random Workload: Reads to random locations on disk, common in database
Sequential Workload: reads a large number of sectors consecutively from
    disk. 
Random is much faster than sequential. People will spend large amount of 
    money on fast transfer and less on cheap but large storage amount 
    (check this)

*** Disk Scheduling ***
By estimating the seek and possible rotational delay of a request, the disk scheduler can know how long
each request will take, and thus (greedily) pick the one that will take the
least time to service first. Thus, the disk scheduler will try to follow the
principle of SJF (shortest job first) in its operation.

*** Shortest Seek time first ***
SSTF or Shorest seek first (SSF) orders the queue of IO requests by track
    picking requests on the nearest tracks to complete first. 

When is SSF not the best pick? 
    When drive geometry is not available (when would it not be avai? 
        servers?) to the host OS rather it sees a block of arrays
    In that case we use, nearest block first(NBF) which schedules 
        the request with the nearest block address first 

    Second, is the issue of starvation: If many requests happen on one
        specific track the other requests, for other tracks, will not be
        given resources.

*** Elevator (SCAN or C-SCAN) ***
An alogorithum that moves back and forth across the disk servicing 
    requests in order across tracks. 

Sweep: a single pass across the disk 
    If a request comes for a block on a track that has already been 
    serviced on this sweep of the disk it is not handled immediately but 
    rather queued until the next sweep 

Scan has many vaients, some, Fscan, Cscan, 

Scan does not represent the best scheduling tech: it does not really
    adhere to shortest job first, it ignored rotation

CRUX: What algorithum can take into account seek and rotation?

*** SPTF: Shortest positioning time first ***
or shortest access time first (SATF) is the solution to the CRUX
    The answer to which sector should a head move to is, it depends.
    THe answer lies in the why does it depends and the details around it
    For this asnwer the depends is, relative time of seeking as compared 
        to rotation. If rotation is faster, then SSTF is good, if seek 
        time is faster then seeking further is better 

--- Livny's Law ---
Almost any question can be answered with "it depends" 

*** Other scheduling issues ***
Where is disk scheduling done?
In older systems, the operating system did all the scheduling; after 
    looking through the set of pending requests, the OS would pick the 
    best one, and issue it to the disk. When that request completed, the 
    next one would be chosen, and so forth. Disks were simpler then, and 
    so was life.

In modern systems, disks can accommodate multiple outstanding requests, 
    and have sophisticated internal schedulers themselves (which can
    implement SPTF accurately; inside the disk controller, all relevant 
    details are available, including exact head position). Thus, the OS 
    scheduler usually picks what it thinks the best few requests are 
    (say 16) and issues them all to disk; the disk then uses its 
    internal knowledge of head position and detailed track layout 
    information to service said requests in the best possible (SPTF) 
    order.


Chapter 38: Redundant Arrays of Inexpensive Disks (RAIDS)
How do we make a faster and larger disk?
Redundant Array of Inexpensive Disks, RAID, a technique to use multiple 
    disks in concert to build faster, bigger, and more reliable systems

Externally a RAID looks like a regular disk. A group of blocks one can
    read and write too. Internally the RAID is a complex beast consisting
    of multiple disks, memory (both volitile and non) and one or more 
    processors to manage the system. 

Advangtages over disk: 
    performance, using mulitple disks in parallel can greatly speed up IO
    times 
    capacity: large data demands large disks
    Reliability: spreading data across multiple disks (without RAIDs) 
        makes the data more vunerable but with some form of Redundancy
        raids can tolerate the loss of a disk and keep operating as if 
        nothing were wrong 
    Transparently: to systems that use them, i.e., a RAID just looks 
        like a big disk to the host system. The beauty of transparency, 
        of course, is that it enables one to simply replace a disk with 
        a RAID and not change a single line of software
    Deployability: RAID enables users and admin to put a RAID to use 
        without worries of software compatability 

*** Interface and RAID internals ***
To the file system a RAID looks like a big, fast, and reliable disk. Just
    as with a single disk, it presents itself as a linear array of blocks

When a file system issues a logical I/O request to the RAID, the RAID 
    internally must calulate which disk (or disks) to access in order to 
    complete the request, and then issue one or more physical I/O's to 
    do so

RAID systems are often external hardware that is connected to the host 
    througn SATA or SCSI. 
Internally it is a complicated system that can consist of a 
    microcontroller that runs firmware to direct the operation of the 
    RAID volite memory such as DRAM to buffer data blocks as they are 
    read/written and in some cases non volitle memory to buffer writes 
    safely and perhaps even specialized logic to perform parity 
    calulations 
Hight level, a RAID is very much a specialized computer system, it has a
    processor, memory, and disks, however, intead of running applications
    it runs specialized software designed to operate the RAID 

*** Fault Model ***
RAIDs are designed to detect and recover from certain kinds of disk 
    faults; thus, knowing exactly which faults to expect is critical in 
    arriving upon a working design.
Fail-Stop: A fault mode that makes disks be in 2 states. a disk can be in
    exactly one of two states: working or failed. With a working disk, 
    all blocks can be read or written. In contrast, when a disk has 
    failed, we assume it is permanently lost.

 Assumptions of fail-stop: It assumes that a failed disk is easily 
    detected. 

*** How to evaulate a RAID ***
Evaulate amoung 3 axes:
    Capacity: Given a set of N disks with B blocks, how much useful 
        capacity is available to clients? 
            Without Redundancy the answer is N*Basic
            If we use mirrioring it is (N*B)/2
    Reliability: How many disk faults can the given design tolerate 
    Performance: Depends on the workload presentenced to the disk array 

*** RAID Level 0: Stripping ***
It serves as an excellent upper-bound on performance and capacity 

The simplest form of stripping will stripe blocks across the disks of 
    the system as follows, figure 38.1
    It is designed to extract the most amount of parallelism from the 
        array when requests are made for contigous chunks of the array 

*** Chunk Size ***
Chunk size mostly affects performance of the array
    a small chunk size implies that many files will get striped across 
    many disks, thus increasing the parallelism of reads and writes to a 
    single file
    A big chunk size, on the other hand, reduces such intra-file 
    parallelism, and thus relies on multiple concurrent requests to 
    achieve high throughput.

determining the “best” chunk size is hard to do, as it requires a
    great deal of knowledge about the workload presented to the disk 
    system

*** Back to Raid-0 analysis ***
From the standpoint of reliability, striping is also perfect, but in the 
    bad way: any disk failure will lead to data loss. Finally, 
    performance is excellent: all disks are utilized, often in parallel, 
    to service user I/O requests.

*** Evaluating RAID Performance***
One can consider 2 performance matrix

first is single-request latency. Understanding the latency of a single 
    I/O request to a RAID is useful as it reveals how much parallelism 
    can exist during a single logical I/O operation.
second is steady-state throughput of the RAID, i.e., the total bandwidth 
    of many concurrent requests.

example in pg 7:
calulate sequential access time:
S = Amount of data/time access 
    time access can be calualted avg seek time+avg totaltional delay
    +transfer of disk rate

random:
R = amount of data (10kb) / time to access

*** RAID lv 1: Mirroring ***
With a mirrored system, we simply make more than one copy of each block 
    in the system; each copy should be placed on a separate disk, of 
    course. By doing so, we can tolerate disk failures.
    When reading you can read from either copy and its the same 
    when writing you need to write to both disks in parallel in order to 
        keep the mirror
    
*** RAID 1 - Analysis ***
From a capacity standpoint, RAID-1 is expensive;
    with the mirroring level = 2, we only obtain half of our peak useful 
    capacity
From a reliability standpoint, RAID-1 does well. It can tolerate the 
    failure of any one disk.

From the perspective of the latency of a single read request, we can see 
    it is the same as the latency on a single disk; all the RAID-1 does 
    is direct the read to one of its copies. 

A write is a little different: it requires two physical writes to 
    complete before it is done. So the time will be slightly longer than 
    a single Disk

*** RAID level 4: Saving Space with Parity *** (look over)
parity block, store the redundant information for that stripe of blocks

*** RAID-4 Analysis ***
From a capacity standpoint, RAID-4 uses 1 disk for parity information 
    for every group of disks it is protecting
    Thus, our useful capacity for a RAID group is (N − 1) · B.

Performance: This time, let us start by analyzing steadystate 
    throughput. Sequential read performance can utilize all of the disks
    except for the parity disk, and thus deliver a peak effective 
    bandwidth of (N − 1) · S MB/s (an easy case).

How to write to RAID-4 if one disk needs to change?
Two methods: 
    The first, known as additive parity, requires us to do the 
        following. To compute the value of the new parity block, read in 
        all of the other data blocks in the stripe in parallel (in the
        example, blocks 0, 2, and 3) and XOR those with the new block (1)
        The problem with this technique is that it scales with the 
        number of disks, and thus in larger RAIDs requires a high number 
        of reads to compute parity
    Second: Subractive Parity: Works in 3 parts 
        First: we read in the ol data, in the disk we want to rewrite, 
        and the old parity. 
        Then, we compare the old and the new data. If they are the same
            then the data and parity stay the same. If they are different
            then we flip the old parity bit to the opposite of its 
            current state (ex parity = 1, then we change to parity=0). 

*** RAID lv 5: Rotating Parity ***
Similar to RAID-4
The effective capacity and failure tolerance of the two levels are 
    identical. So are sequential read and write performance. The latency 
    of a single request (whether a read or a write) is also the same as 
    RAID-4.

Random read performance is a little better, because we utilize all disks 
Finally, random write performance improves noticeably over RAID-4, as it 
    allows for parallelism across requests.

---- Figure 38.8 in page 15 has a breakdown of the differences ----

Because RAID-5 is basically identical to RAID-4 except in the few cases
    where it is better, it has almost completely replaced RAID-4 in the 
    marketplace.

The only place where it has not is in systems that know they will
    never perform anything other than a large write, thus avoiding the 
    smallwrite problem altogether [HLM94]; in those cases, RAID-4 is 
    sometimes used as it is slightly simpler to build.

To conclude, if you strictly want performance and do not care about
    reliability, striping is obviously best.

If, however, you want random I/O performance and reliability, mirroring 
    is the best;  the cost you pay is in lost capacity

If capacity and reliability are your main goals, then RAID5 is the 
    winner; the cost you pay is in small-write performance. 

Finally, if you are always doing sequential I/O and want to maximize 
    capacity, RAID-5 also makes the most sense.

