Chapter 36: I/o
CPU is attached to the main memory of the system of the system via
    some kind of memory bus or interconnect. Some of the devices are 
    to the system via a general I/O bus, which in modern systems are 
    call PCI graphics and some other high preformance I/O devices might 
    be found there 
Lower down are one or more of peripheral bus, such as SCSI, Sata, or USB
    USB devices include disks, mouse, or keyboard these are slow 

We need heirarchical structures simply for physics and cost. The faster a
    bus is the shorter is must be, thus a high performance memory bus doe
    not have much room to plug devices into 

Engineering a bus for high performance is costly so a hierarchical 
    approach where components that demand high performance (liek 
    graphics) cards are closer to the CPU. lower performance are 
    farther away 

*** A Canonical Device ***
A device has two important components. The first is hardware interface 
    and the second is internal structure.
    Hardware Interface: hardware must also have some kind of interface 
        that allows the system software to control its operation. The 
        hardware must have some kind of interface that allows the system 
        software to control its operation. Thus all devices have a 
        specified interface and protocol for typical interaction
    Internal Structure: This part of the device is implementation 
        specific and is responsible for implemenating the abstraction 
        the device presents to the system. Very simple devices will have 
        one or a few hardware chips to implement their functionality; 
        more complex devices will include a simple CPU, some general 
        purpose memory, and other device-specific chips to get their job 
        done.

*** Canonical protocol ***
Device interface is compromised of three registers:
    Status register: can be read to see the current status of the device 
    Command Register: To tell the device to perform a certain task 
    Data Register: pass data to the device or get data from the device 
Polling: OS waits until the device is ready to recieve a command by 
    repeatedly reading the status register (OS is asking whats going 
    on)
Protocols have 4 steps:
Typical interaction that the OS might have with a device is: 
    first, OS waits until the device is ready to recieve command
    second, OS sends data down to the register
    third, OS writes a command to the command register, doing so lets 
        the device know data is present and it can start working on the 
        command
    fourth, OS waits for the device to finish by again polling in a loop
        waiting to see if it is finished (it may then get an error code)


Chapter 37: HardDisk Drives
Main form of persistent data storage 

*** Interface ***
Drive consists of a large number of sectors (512-byte blocks) each of 
    which can be written or read 

The sectors are numbered from 0 to n -1 on a disk with n sectors. We can
    view the disk as an array of sectors, 0 to n-1 is thus the address
    space 

The only guarentee drive manufacturers make is that a single 512-byte 
    write is atomic, it will either complete in its entireety or it 
    wont comeplete at all, thus if a power loss occurs only a portion of
    a larger write may complete (torn write)

THere is an "unwritten contract" of disk drives. It assumes that 
    accessing two blocks near one another is faster than accessing block
    that are farther away. Thus accessing blocks in a contiguous
    (touching) chunks is the fastests access mode and usually much 
    faster than a random access pattern

*** Basic Geometry ***
Platter: a circular surface on which data is stores persistently by
    inducing magnetic changes to it. Drives may have one or more platters
    Each platter has 2 sides, each side called a surface
Spindle: The platters are bound together by the spindle, its connected to
    the motor that spins the platter around while the drive is powered on
    and it spins at a constant fixed rate. (7200 to 15000 RPM). Single 
    rotation takes around 6ms
Track: Data is encoded on each surface in concentric circles of sectors,
    we call it a track. a surface holds thousands of tracks
Disk Head/Disk Arm: The head is the hardhard that read/write onto the 
    surface. The head is moved around using the disk arm 

*** Simple Disk Drive ***
simple disk with a single track (Figure 37.1). This track has just 12 
    sectors, each of which is 512 bytes in size (our typical sector 
    size, recall) and addressed therefore by the numbers 0 through 11. 
    The single platter we have here rotates around the spindle, to which 
    a motor is attached.

*** SIngle track latency: The rotational Delay ***
To process a request hour disk just waits for the desired sector to 
    rotate under the disk head.
Rotational Delay: THe process of waiting for a sector to rotate under 
    the head. The delay can be described by (R/2)

*** Multiple tracks: seek time ***
Seek: To service a read the drive has to first move the disk arm to the
    correct track, this process is called seek
Seek has multiple phases: 
    Acceleration: the disk arm gets moving, then 
    coasting: as the arm is moving at full speed 
    deceleration: arm slows down 
    setting: the head is carefully positioned over the correct track 

After the seek, the disk arm has positioned the head over the right
    track.

The settling time is often quite significant .5 to .2 ms 

After the desired sector is under the disk head the final phase of I/O 
    will take place called transfer
    transfer: data is either read from or written to the surface

*** Other details ***
Track Skew: to make sure that sequential reads can be properly serviced 
    even when crossing track boundarie. 

Without the skew the head would be moved to the next track but the 
    desired next block would have passed thus needing to do a full 
    rotation

multi-zoned disk drives, where the disk is organized into multiple 
    zones, and where a zone is consecutive set of tracks on a surface.

cache, for historical reasons sometimes called a track buffer. This
    cache is just some small amount of memory (usually around 8 or 16 
    MB) which the drive can use to hold data read from or written to the 
    disk. 

When should the disk acknowledge the write has completed when it puts the
    data in its memory or after the write has been written to disk. 
    Write back(immidiate reporting): Complete when data is in memory 
    Write through: Complete when data is written to disk 

I/O time: Doing math for disk performance (pg 7)
T(I/O) = T(seek) + T(rotation) + T(transfer)
Random Workload: Reads to random locations on disk, common in database
Sequential Workload: reads a large number of sectors consecutively from
    disk. 
Random is much faster than sequential. People will spend large amount of 
    money on fast transfer and less on cheap but large storage amount 
    (check this)

*** Disk Scheduling ***
By estimating the seek and possible rotational delay of a request, the disk scheduler can know how long
each request will take, and thus (greedily) pick the one that will take the
least time to service first. Thus, the disk scheduler will try to follow the
principle of SJF (shortest job first) in its operation.

*** Shortest Seek time first ***
SSTF or Shorest seek first (SSF) orders the queue of IO requests by track
    picking requests on the nearest tracks to complete first. 

When is SSF not the best pick? 
    When drive geometry is not available (when would it not be avai? 
        servers?) to the host OS rather it sees a block of arrays
    In that case we use, nearest block first(NBF) which schedules 
        the request with the nearest block address first 

    Second, is the issue of starvation: If many requests happen on one
        specific track the other requests, for other tracks, will not be
        given resources.

*** Elevator (SCAN or C-SCAN) ***
An alogorithum that moves back and forth across the disk servicing 
    requests in order across tracks. 

Sweep: a single pass across the disk 
    If a request comes for a block on a track that has already been 
    serviced on this sweep of the disk it is not handled immediately but 
    rather queued until the next sweep 

Scan has many vaients, some, Fscan, Cscan, 

Scan does not represent the best scheduling tech: it does not really
    adhere to shortest job first, it ignored rotation

CRUX: What algorithum can take into account seek and rotation?

*** SPTF: Shortest positioning time first ***
or shortest access time first (SATF) is the solution to the CRUX
    The answer to which sector should a head move to is, it depends.
    THe answer lies in the why does it depends and the details around it
    For this asnwer the depends is, relative time of seeking as compared 
        to rotation. If rotation is faster, then SSTF is good, if seek 
        time is faster then seeking further is better 

--- Livny's Law ---
Almost any question can be answered with "it depends" 

*** Other scheduling issues ***
Where is disk scheduling done?
In older systems, the operating system did all the scheduling; after 
    looking through the set of pending requests, the OS would pick the 
    best one, and issue it to the disk. When that request completed, the 
    next one would be chosen, and so forth. Disks were simpler then, and 
    so was life.

In modern systems, disks can accommodate multiple outstanding requests, 
    and have sophisticated internal schedulers themselves (which can
    implement SPTF accurately; inside the disk controller, all relevant 
    details are available, including exact head position). Thus, the OS 
    scheduler usually picks what it thinks the best few requests are 
    (say 16) and issues them all to disk; the disk then uses its 
    internal knowledge of head position and detailed track layout 
    information to service said requests in the best possible (SPTF) 
    order.


Chapter 38: Redundant Arrays of Inexpensive Disks (RAIDS)
How do we make a faster and larger disk?
Redundant Array of Inexpensive Disks, RAID, a technique to use multiple 
    disks in concert to build faster, bigger, and more reliable systems

Externally a RAID looks like a regular disk. A group of blocks one can
    read and write too. Internally the RAID is a complex beast consisting
    of multiple disks, memory (both volitile and non) and one or more 
    processors to manage the system. 

Advangtages over disk: 
    performance, using mulitple disks in parallel can greatly speed up IO
    times 
    capacity: large data demands large disks
    Reliability: spreading data across multiple disks (without RAIDs) 
        makes the data more vunerable but with some form of Redundancy
        raids can tolerate the loss of a disk and keep operating as if 
        nothing were wrong 
    Transparently: to systems that use them, i.e., a RAID just looks 
        like a big disk to the host system. The beauty of transparency, 
        of course, is that it enables one to simply replace a disk with 
        a RAID and not change a single line of software
    Deployability: RAID enables users and admin to put a RAID to use 
        without worries of software compatability 

*** Interface and RAID internals ***
To the file system a RAID looks like a big, fast, and reliable disk. Just
    as with a single disk, it presents itself as a linear array of blocks

When a file system issues a logical I/O request to the RAID, the RAID 
    internally must calulate which disk (or disks) to access in order to 
    complete the request, and then issue one or more physical I/O's to 
    do so

RAID systems are often external hardware that is connected to the host 
    througn SATA or SCSI. 
Internally it is a complicated system that can consist of a 
    microcontroller that runs firmware to direct the operation of the 
    RAID volite memory such as DRAM to buffer data blocks as they are 
    read/written and in some cases non volitle memory to buffer writes 
    safely and perhaps even specialized logic to perform parity 
    calulations 
Hight level, a RAID is very much a specialized computer system, it has a
    processor, memory, and disks, however, intead of running applications
    it runs specialized software designed to operate the RAID 

*** Fault Model ***
RAIDs are designed to detect and recover from certain kinds of disk 
    faults; thus, knowing exactly which faults to expect is critical in 
    arriving upon a working design.
Fail-Stop: A fault mode that makes disks be in 2 states. a disk can be in
    exactly one of two states: working or failed. With a working disk, 
    all blocks can be read or written. In contrast, when a disk has 
    failed, we assume it is permanently lost.

 Assumptions of fail-stop: It assumes that a failed disk is easily 
    detected. 

*** How to evaulate a RAID ***
Evaulate amoung 3 axes:
    Capacity: Given a set of N disks with B blocks, how much useful 
        capacity is available to clients? 
            Without Redundancy the answer is N*Basic
            If we use mirrioring it is (N*B)/2
    Reliability: How many disk faults can the given design tolerate 
    Performance: Depends on the workload presentenced to the disk array 

*** RAID Level 0: Stripping ***
It serves as an excellent upper-bound on performance and capacity 

The simplest form of stripping will stripe blocks across the disks of 
    the system as follows, figure 38.1
    It is designed to extract the most amount of parallelism from the 
        array when requests are made for contigous chunks of the array 

*** Chunk Size ***
Chunk size mostly affects performance of the array
    a small chunk size implies that many files will get striped across 
    many disks, thus increasing the parallelism of reads and writes to a 
    single file
    A big chunk size, on the other hand, reduces such intra-file 
    parallelism, and thus relies on multiple concurrent requests to 
    achieve high throughput.

determining the “best” chunk size is hard to do, as it requires a
    great deal of knowledge about the workload presented to the disk 
    system

*** Back to Raid-0 analysis ***
From the standpoint of reliability, striping is also perfect, but in the 
    bad way: any disk failure will lead to data loss. Finally, 
    performance is excellent: all disks are utilized, often in parallel, 
    to service user I/O requests.

*** Evaluating RAID Performance***
One can consider 2 performance matrix

first is single-request latency. Understanding the latency of a single 
    I/O request to a RAID is useful as it reveals how much parallelism 
    can exist during a single logical I/O operation.
second is steady-state throughput of the RAID, i.e., the total bandwidth 
    of many concurrent requests.

example in pg 7:
calulate sequential access time:
S = Amount of data/time access 
    time access can be calualted avg seek time+avg totaltional delay
    +transfer of disk rate

random:
R = amount of data (10kb) / time to access

*** RAID lv 1: Mirroring ***
With a mirrored system, we simply make more than one copy of each block 
    in the system; each copy should be placed on a separate disk, of 
    course. By doing so, we can tolerate disk failures.
    When reading you can read from either copy and its the same 
    when writing you need to write to both disks in parallel in order to 
        keep the mirror
    
*** RAID 1 - Analysis ***
From a capacity standpoint, RAID-1 is expensive;
    with the mirroring level = 2, we only obtain half of our peak useful 
    capacity
From a reliability standpoint, RAID-1 does well. It can tolerate the 
    failure of any one disk.

From the perspective of the latency of a single read request, we can see 
    it is the same as the latency on a single disk; all the RAID-1 does 
    is direct the read to one of its copies. 

A write is a little different: it requires two physical writes to 
    complete before it is done. So the time will be slightly longer than 
    a single Disk

*** RAID level 4: Saving Space with Parity *** (look over)
parity block, store the redundant information for that stripe of blocks

*** RAID-4 Analysis ***
From a capacity standpoint, RAID-4 uses 1 disk for parity information 
    for every group of disks it is protecting
    Thus, our useful capacity for a RAID group is (N − 1) · B.

Performance: This time, let us start by analyzing steadystate 
    throughput. Sequential read performance can utilize all of the disks
    except for the parity disk, and thus deliver a peak effective 
    bandwidth of (N − 1) · S MB/s (an easy case).

How to write to RAID-4 if one disk needs to change?
Two methods: 
    The first, known as additive parity, requires us to do the 
        following. To compute the value of the new parity block, read in 
        all of the other data blocks in the stripe in parallel (in the
        example, blocks 0, 2, and 3) and XOR those with the new block (1)
        The problem with this technique is that it scales with the 
        number of disks, and thus in larger RAIDs requires a high number 
        of reads to compute parity
    Second: Subractive Parity: Works in 3 parts 
        First: we read in the ol data, in the disk we want to rewrite, 
        and the old parity. 
        Then, we compare the old and the new data. If they are the same
            then the data and parity stay the same. If they are different
            then we flip the old parity bit to the opposite of its 
            current state (ex parity = 1, then we change to parity=0). 

*** RAID lv 5: Rotating Parity ***
Similar to RAID-4
The effective capacity and failure tolerance of the two levels are 
    identical. So are sequential read and write performance. The latency 
    of a single request (whether a read or a write) is also the same as 
    RAID-4.

Random read performance is a little better, because we utilize all disks 
Finally, random write performance improves noticeably over RAID-4, as it 
    allows for parallelism across requests.

---- Figure 38.8 in page 15 has a breakdown of the differences ----

Because RAID-5 is basically identical to RAID-4 except in the few cases
    where it is better, it has almost completely replaced RAID-4 in the 
    marketplace.

The only place where it has not is in systems that know they will
    never perform anything other than a large write, thus avoiding the 
    smallwrite problem altogether [HLM94]; in those cases, RAID-4 is 
    sometimes used as it is slightly simpler to build.

To conclude, if you strictly want performance and do not care about
    reliability, striping is obviously best.

If, however, you want random I/O performance and reliability, mirroring 
    is the best;  the cost you pay is in lost capacity

If capacity and reliability are your main goals, then RAID5 is the 
    winner; the cost you pay is in small-write performance. 

Finally, if you are always doing sequential I/O and want to maximize 
    capacity, RAID-5 also makes the most sense.



Chapter 39: File Directories 
In this chapter we add one more cirtical piece to the virtualization puzzle:
    persistent storage, such as a harddtive or SSD, store information permanently
    keeps data after power is off, unlike ram. 

*** File and Directories ***
File, is a simple linear array of bytes each of which can read or write. Each
    file has some kind of low-level name usually a number of some kind, often 
    the user is not aware of this name. the lower level name is referrd to as 
    inode-number 

    In most systems the OS does not know what kind of file you storage, .txt, pic
    or code it just stores info so when you try to retrieve it, its exactly 
    what you put inside of it 

Directory, like a file, has a lower-level name, inode number, but its contents 
    specific, it contains a list of, user readable names, and the inode number. 
    For example if we have a file called "foo" with the inode number "10" it 
    would be stored in the directory as ("foo", "10"), with its lower and 
    user readable name mapped together. 

You can store directories within directories so it would look like a 
    hierarchy with all the files and other directories stemming off
    a root directory 

*** File system Interface ***
To remove files we would need to call unlink()

*** Creating Files ***
To create a file you need to call open() and the flag "O_CREAT" in order 
    for a program to create a new file. 

---Code example that create a file called "foo" in the current working
    directory 
    int fd = open("foo", O_CREAT | O_WRONLY | O_TRUNC, S_IRUSR | S_IWUSR);

The routine open() takes a number of different flags. In this example, 
    the second parameter creates the file (O CREAT) if it does not exist,
    ensures that the file can only be written to (O WRONLY), and, if the file
    already exists, truncates it to a size of zero bytes thus removing any 
    existing content (O TRUNC). The third parameter specifies permissions, in this
    case making the file readable and writable by the owner.

open() returns a file descriptor, its an integer, private per process, that is used
    in UNIX to access files, thus once a file is opened you will use the descriptor
    to read and write, assuming you have permissions for it 

The file descriptor is a capbility, a handle that gives you the power to perform
    certain operations. It can also be thought up as a pointer to an object 
    of type file. once you have such an object, you can call other “methods” 
    to access the file, like read() and write()

A simple array (with a maximum of NOFILE open files) tracks which files are 
    opened on a per-process basis. Each entry of the array is actually just a pointer 
    to a struct file, which will be used to track information about the file being 
    read or written

*** Reading and Writing Files ***
You can use strace to see what a file is doing when running a command on it. 39.4
    has an example of what happens when you run cat foo, foo is a txt file with 
    the world "hello" in it

each running process already has three files open, standard input (which the 
    process can read to receive input), standard output (which the process 
    can write to in order to dump information to the screen), and standard 
    error (which the process can write error messages to).

These are represented by file descriptors 0, 1, and 2, respectively. 
    Thus, when you first open another file (as cat does above),
    it will almost certainly be file descriptor 3

After open works cat uses read() system call to repeatedly read some bytes from a file. 
    The first argument to read() is the file descriptor, thus telling the file system 
        which file to read; a process can of course have multiple files open at once, and 
        thus the descriptor enables the operating system to know which file a particular
        read refers to
    The second argument points to a buffer where the result of the read() will be placed 
        in the system-call trace above, strace sshows the results of the read in this spot 
        ("hello") 
    The third argument is the size of the buffer which in this case is 4kb. The call to 
        read returns successfully, here returning the number the number of bytes it reads 
        (6, which includes 5 for the letters in the word hello and one for end of line
        marker)

At this point, you see another interesting result of the strace: a single call to the write() 
    system call, to the file descriptor 1, which is the stdout argument 

The cat program then tries to read more from the file, but since there are no bytes left in 
    the file, the read() returns 0 and the program knows that this means it has read the entire 
    file. Thus, the program calls close() to indicate that it is done with the file “foo”, 
    passing in the corresponding file descriptor. The file is thus closed, and the reading 
    of it thus complete.

Writing a file is accomplished via a similar set of steps. First, a file is opened for writing, 
    then the write() system call is called, perhaps repeatedly for larger files, and then close().

*** Reading And Writing, But Not Sequentially ***
There are times you need to reead or write to a file that is not at the
    begining or end of a file 

In the book lseek() was introducedm it takes a few different arguments 
    First arugment, it takes a file descriptor, which tells lseek() what 
        file its going to be working on
    The second argument, is the offset, which positions the file offset 
        to a particular location within the file. 
    The third argument, called whence determins exactly how the seek is 
        preformed 
        If whence is SEEK_SET, the offset is set to offset bytes.
        If whence is SEEK_CUR, the offset is set to its current
            location plus offset bytes.
        If whence is SEEK_END, the offset is set to the size of
            the file plus offset bytes.
    
*** Shared File Table Enteries: Fork() and dup() ***
In many cases the mapping of file descriptor to an entry in the open file
    table is a 1 to 1 mapping. 

if some other process reads the same file at the same time, each will 
    have its own entry in the open file table.

However, there are a few interesting cases where an entry in the open
    file table is shared. 
    One of those cases occurs when a parent process creates a child 
        process with fork().

Figure 39.3 shows the relationships that connect each process’s private
    descriptor array, the shared open file table entry, and the 
    reference from it to the underlying file-system inode
    If a child process changes the offset using lseek() it will affect 
        the offset of the parent process
    
One other interesting, and perhaps more useful, case of sharing occurs
    with the dup() system call (and its cousins, dup2() and dup3()).

The dup() call allows a process to create a new file descriptor that 
    refers to the same underlying open file as an existing descriptor 

*** Writing Immediately with fsync ***
There are some applications, like databases, that requrie something 
    immidiate than fwrite() can do
To help with this there ar a few API's that can help
    fsync(int fd): when a process, for a specific file descriptor calls 
        it the file system responds by forcing all dirty(not yet 
        written) data to disk, for the file referred to by the specific 
        file descriptor 
        The routine returns once all of the writes are complete 

Example code on page 12:
    The code opens the file foo, writes a single chunk of data to it, 
    and then calls fsync() to ensure the writes are forced immediately 
    to disk. Once the fsync() returns, the application can safely move 
    on, knowing that the data has been persisted (if fsync() is 
    correctly implemented, that is)

Interestingly, this sequence does not guarantee everything that you
    might expect; in some cases, you also need to fsync() the directory 
    that contains the file foo. Adding this step ensures not only that 
    the file itself is on disk, but that the file, if newly created, 
    also is durably a part of the directory

*** Renaming Files ***
One interesting guarantee provided by the rename() call is that it is
    (usually) implemented as an atomic call with respect to system 
    crashes; if the system crashes during the renaming, the file will 
    either be named the old name or the new name, and no odd in-between 
    state can arise.

*** Getting information about files ***
Metadata: Information about the file 
To see the metadata for a certain file, we can use the stat() or fstat() 
    system calls. These calls take a pathname (or file descriptor) to a 
    file and fill in a stat structure
Ex:
prompt> echo hello > file
prompt> stat file
File: ‘file’
Size: 6 Blocks: 8 IO Block: 4096 regular file
Device: 811h/2065d Inode: 67158084 Links: 1
Access: (0640/-rw-r-----) Uid: (30686/remzi)
Gid: (30686/remzi)
Access: 2011-05-03 15:50:20.157594748 -0500
Modify: 2011-05-03 15:50:20.157594748 -0500
Change: 2011-05-03 15:50:20.157594748 -0500

*** Removing Files ***
When removing files you need rm but what system call is used?
    strace rm foo
    unlink("foo") = 0

*** Making Directories ***
Note you can never write to a directory directly. Because the format of 
    the directory is considered file system metadata, the file system 
    considers itself responsible for the integrity of directory data; 
    thus, you can only update a directory indirectly by, for example, 
    creating files, directories, or other object types within it

To create a directory, a single system call, mkdir(), is available. The
    eponymous mkdir program can be used to create such a directory
Ex.
strace mkdir foo
mkdir("foo", 0777)

When such a directory is created, it is considered “empty”, although it
    does have a bare minimum of contents. Specifically, an empty 
    directory has two entries: one entry that refers to itself, and one 
    entry that refers to its parent. The former is referred to as the “.
    ” (dot) directory, and the latter as “..”

you can see directories by passing the -al flag in ls 
ls -al 
total 8
drwxr-x--- 2 remzi remzi 6 Apr 30 16:17 ./
drwxr-x--- 26 remzi remzi 4096 Apr 30 16:17 ../

*** Reading Directories ***
Instead of just opening a directory as if it were a file, we instead use
    a new set of calls. they include opendir(), readdir(), and closedir()
ex. 
int main(int argc, char *argv[]) {
    DIR *dp = opendir(".");
    assert(dp != NULL);
    struct dirent *d;
    while ((d = readdir(dp)) != NULL) {
        printf("%lu %s\n", (unsigned long) d->d_ino,
            d->d_name);
    }
    closedir(dp);
    return 0;
}

struct dirent {
    char d_name[256]; // filename
    ino_t d_ino; // inode number
    off_t d_off; // offset to the next dirent
    unsigned short d_reclen; // length of this record
    unsigned char d_type; // type of file
};

*** Deleting Directories ***
rmdir() has the requirement that the directory be empty (i.e., only has 
    “.” and “..” entries) before it is deleted. If you try to delete a 
    non-empty directory, the call to rmdir() simply will fail.

*** Hard Links ***
We now come back to the mystery of why removing a file is performed
    via unlink(), by understanding a new way to make an entry in the
    file system tree, through a system call known as link(). The link()
    system call takes two arguments, an old pathname and a new one; when
    you “link” a new file name to an old one, you essentially create 
    another way to refer to the same file. The command-line program ln 
    is used to do this, as we see in this example:
ex. 
echo hello > file
cat file
out:hello
ln file file2
cat file2
out:hello

Here we created a file with the word “hello” in it, and called the file
    file2. We then create a hard link to that file using the ln program. 
    After this, we can examine the file by either opening file or file2.

The way link() works is that it simply creates another name in the
    directory you are creating the link to, and refers it to the same 
    inode number (i.e., low-level name) of the original file. 
    if you make changes to either one it changes both
    --you can check to see they have the same inode num using the -i flag
        with ls (ls -i)

When removing a file (rm) you also call unlink as well to remove all 
    links to that file
ex.
prompt> echo hello > file
prompt> stat file
... Inode: 67158084 Links: 1 ...
prompt> ln file file2
prompt> stat file
... Inode: 67158084 Links: 2 ...
prompt> stat file2
... Inode: 67158084 Links: 2 ...
prompt> ln file2 file3
prompt> stat file
... Inode: 67158084 Links: 3 ...
prompt> rm file
prompt> stat file2
... Inode: 67158084 Links: 2 ...
prompt> rm file2
prompt> stat file3
... Inode: 67158084 Links: 1 ...
prompt> rm file3

*** Symblic Links ***
There is one other type of link that is really useful, and it is called a
    symbolic link or sometimes a soft link.

To create such a link, you can use the same program ln, but with the
-s flag. Here is an example:
echo hello > file
ln -s file file2
cat file2
hello

However, beyond this surface similarity, symbolic links are actually
    quite different from hard links. The first difference is that a 
    symbolic link is actually a file itself, of a different type.

Running ls also reveals this fact. If you look closely at the first 
    character of the long-form of the output from ls, you can see that 
    the first character in the left-most column is a '-' for regular 
    files, a 'd' for directories, and an 'l' for soft links
ex. 
prompt> ls -al
drwxr-x--- 2 remzi remzi 29 May 3 19:10 ./
drwxr-x--- 27 remzi remzi 4096 May 3 15:14 ../
-rw-r----- 1 remzi remzi 6 May 3 19:10 file
lrwxrwxrwx 1 remzi remzi 4 May 3 19:10 file2 -> file

Finally, because of the way symbolic links are created, they leave the
    possibility for what is known as a dangling reference:
prompt> echo hello > file
prompt> ln -s file file2
prompt> cat file2
hello
prompt> rm file
prompt> cat file2
cat: file2: No such file or directory

As you can see in this example, quite unlike hard links, removing the
    original file named file causes the link to point to a pathname that 
    no longer exists. It also adds permissions to owner

*** Permission Bits And Access Control Lists ***
 ex.
prompt> ls -l foo.txt
-rw-r--r-- 1 remzi wheel 0 Aug 24 16:29 foo.txt
The first character here just shows the type of the file: - for
    a regular file (which foo.txt is), d for a directory, l for a 
    symbolic link, and so forth

Permissions consist of 3 groupings, owner, group, and other
    The abilities the owner, group member, or others can have include 
    the ability to read the file, write it, or execute it.

The owner of the file can readily change these permissions, for example 
    by using the chmod command (to change the file mode). 

*** Making And Mounting A File System *** (look over)
To make a file system, most file systems provide a tool, usually 
    referred to as mkfs (pronounced “make fs”), that performs exactly 
    this task.

The idea is as follows: give the tool, as input, a device (such as a    
    disk partition, e.g., /dev/sda1) and a file system type (e.g., ext3)
    , and it simply writes an empty file system, starting with a root 
    directory, onto that disk partition.

For the file system to be accessible within the uniform file system is 
    needs the mount program (which makes the underlying system call 
    mount() to do the real work). What mount does, quite simply is take 
    an existing directory as a target mount point and essentially paste 
    a new file system onto the directory tree at that point.

*** Summary on page 27 ***
ASIDE: KEY FILE SYSTEM TERMS
• A file is an array of bytes which can be created, read, written, and
deleted. It has a low-level name (i.e., a number) that refers to it
uniquely. The low-level name is often called an i-number.

• A directory is a collection of tuples, each of which contains a
human-readable name and low-level name to which it maps. Each
entry refers either to another directory or to a file. Each directory
also has a low-level name (i-number) itself. A directory always has
two special entries: the . entry, which refers to itself, and the ..
entry, which refers to its parent.

• A directory tree or directory hierarchy organizes all files and 
directories into a large tree, starting at the root.

• To access a file, a process must use a system call (usually, open())
to request permission from the operating system. If permission is
granted, the OS returns a file descriptor, which can then be used
for read or write access, as permissions and intent allow.

• Each file descriptor is a private, per-process entity, which refers to
an entry in the open file table. The entry therein tracks which file
this access refers to, the current offset of the file (i.e., which part
of the file the next read or write will access), and other relevant
information.

• Calls to read() and write() naturally update the current offset;
otherwise, processes can use lseek() to change its value, enabling
random access to different parts of the file.

• To force updates to persistent media, a process must use fsync()
or related calls. However, doing so correctly while maintaining
high performance is challenging [P+14], so think carefully when
doing so.

• To have multiple human-readable names in the file system refer to
the same underlying file, use hard links or symbolic links. Each
is useful in different circumstances, so consider their strengths and
weaknesses before usage. And remember, deleting a file is just 
performing that one last unlink() of it from the directory hierarchy.

• Most file systems have mechanisms to enable and disable sharing.
A rudimentary form of such controls are provided by permissions
bits; more sophisticated access control lists allow for more precise
control over exactly who can access and manipulate information.
