Chapter 17 : Free Space Management

Free space is equal sized and kept in a list. When a client requests one of them, return the first entry 
Free-space management becomes more difficult when the free space you are managing consists of variable-sizedunit
   This arises in a user level memory allocation library (malloc() and free()
   Also in OS physical memory when using segmentation to implement virtual memory 

External Frangmentation: the free space gets chopped into little pieces of different sizes and is thus frangemented. 
   Since it is fragmented all the pieces do not amount to a whole chunk on memory 

***Assumptions***
When using malloc() it takes size, which is the number of bytes 
   requested by the application; it hands back a pointer, or a 
   void pointer, to a region of that size or greater. 
The complementary routine void free() takes a pointer and frees the 
corresponding chunk

malloc() manages heap library and the generic data structures that are 
   used to manage free space in heap is some kind of free list 
   They contain references to all the free chunks of space in the 
   managed region of memory.

Allocators could have the problem of internal and External
   Frangmentation 
   internal Frangmentation: if an allocator hands out chunks of
   memory bigger than that requested, any unasked for (and thus unused)
   space in such a chunk is considered internal fragmentation (because the waste occurs inside the allocated unit)

We’ll also assume that once memory is handed out to a client, it cannot
   be relocated to another location in memory

allocator manages a contiguous region of bytes. In some cases, an 
   allocator could ask for that region to grow; for example, a 
   user-level memory-allocation library might call into the kernel to 
   grow the heap (via a system call such as sbrk) when it runs out
   of space

*** Low Level Mechanism ***
Splitting and Coalescing 
   splitting: it will find a free chunk of memory that can satisfy the 
   request and split it into two. The first chunk it will return to the 
   caller; the second chunk will remain on the list.
   if a request for 1 byte were made,
   and the allocator decided to use the second of the two elements on 
   the list to satisfy the request, the call to malloc() would return 20 
   (the address of the 1-byte allocated region)

A corollary mechanism found in many allocators is known as coalescing of 
   free space. while the entire heap is now free, it is seemingly
   divided into three chunks of 10 bytes each. Thus, if a user requests 
   20 bytes, a simple list traversal will not find such a free chunk, 
   and return failure.
In order to avoid the problem is fragmenting the memory the allocator 
   coalesces free space in memory. look carefully at the addresses of 
   the chunk you are returning as well as the nearby chunks of free 
   space; if the newlyfreed space sits right next to one (or two, as in 
   this example) existing free chunks, merge them into a single larger 
   free chunk.

***Tracking the size of allocated regions***
Free() determines the size of the pointer that its trying to free byn 
   using malloc() library. Most allocators store a big of information in 
   a header block which is kept in memory, right before the handed out 
   memory. (figure 17.1) ptr = malloc(20) example. The header contains a 
   magic number to provide additional integrity checking. When the user 
   calls free(prt) the library then uses simple pointer math to figure 
   out where the header begins. The library can tehn determine if the 
   magic number matches the expected value as a sanity check and 
   caluclate the size of the newly-freed region via simple math. 
      NOTE: the size of the free region is the size of the header plus 
      the size of the space allocated to the user. Thus, when a user 
      requests N bytes of memory, the library does not search for a free 
      chunk of size N; rather,it searches for a free chunk of size N 
      plus the size of the header.  

***Embedding A free List****
You need to build the list inside the free space itself. We are assuming 
   that the heap is built within some free space acquired via a call to 
   the system call mmap ();this is not the only way to build such a heap 
   but serves us well in this example (pg6)
   Now, let’s imagine that a chunk of memory is requested, say of size
   100 bytes. To service this request, the library will first find a 
   chunk that is large enough to accommodate the request; because there 
   is only one free chunk (size: 4088), this chunk will be chosen. Then, 
   the chunk will be split into two: one chunk big enough to service the 
   request (and header, as described above), and the remaining free 
   chunk. 
When free memory is franmented you need you merge neighboring chunks. 
   When finished the heap will be whole again 

When the heap runs out of space thre simple approach is to just fail, in 
   some examples its the only opinion but thus returns null. To request 
   more memory from the OS the system call sbrk is used. To service
   the sbrk request, the OS finds free physical pages, maps them into the
   address space of the requesting process, and then returns the value of
   the end of the new heap; at that point, a larger heap is available, and the request can be successfully serviced.

***Basic Strategies***
The ideal allocator is both fast and minimizes fragmentation. 
   Unfortunately, because the stream of allocation and free requests can 
   be arbitrary (after all, they are determined by the programmer)

   *** Best Fit
   first, search through the free list and find chunks of free memory 
   that are as big or bigger than the requested size. Then, return the 
   one that is the smallest in that group of candidates; by returning a 
   block that is close to what the user asks, best fit tries to reduce 
   wasted space
      Best Fit cost
         However, there is a cost; naive implementations pay a heavy 
         performance penalty when performing an exhaustive search for 
         the correct free block.
   ***Worst Fit
   opposite of best fit; find the largest chunk and return the requested 
   amount; keep the remaining (large) chunk on the free list. Worst fit 
   tries to thus leave big chunks free instead of lots of small chunks 
   that can arise from a best-fit approach.
      Worst Fit cost
         Once again, however, a full search of free space is required, 
         and thus this approach can be costly. 
   *** First Fit
   finds the first block that is big enough and returns the requested   
   amount to the user. First fit has the advantage of speed — no 
   exhaustive search of all the free spaces are necessary — 
   Thus, how the allocator manages the free list’s order becomes an 
   issue. One approach is to use address-based ordering;
   by keeping the list ordered by the address of the free space, 
   coalescing becomes easier, and fragmentation tends to be reduced.  
      First Fit cost 
         but sometimes pollutes the beginning of the free list with 
         small objects.
   *** Next Fit
   Similar to first fit but  keeps an extra pointer to the location 
   within the list where one was looking last.
      Next Fit cost
         The performance of such an approach is quite
         similar to first fit, as an exhaustive search is once again 
         avoided.

***Segregated Lists***
The basic idea is simple: if a particular application
   has one (or a few) popular-sized request that it makes, keep a 
   separate list just to manage objects of that size; all other requests 
   are forwarded to a more general memory allocator
This approach introduces new complications into a system as well. For 
   example, how much memory should one dedicate to the pool of memory 
   that serves specialized requests 
Slab Allocator used to solve this issue by; Specifically, when the 
   kernel boots up, it allocates a number of object caches for kernel 
   objects that are likely to be requested frequently (such as
   locks, file-system inodes, etc.); the object caches thus are each 
   segregated free lists of a given size and serve memory allocation and 
   free requests quickly.  When a given cache is running low on free 
   space, it requests some slabs of memory from a more general memory 
   allocator

***Binary Buddy***
free memory is first conceptually thought of as one big space of size 2 
   N. When a request for memory is made, the search for
   free space recursively divides free space by two until a block that 
   is big enough to accommodate the request is found (and a further 
   split into two would result in a space that is too small).
      *** cost
         ; note that this scheme can suffer from internal fragmentation, 
         as you are only allowed to give out power-of-two-sized blocks. 
Binary checks each block and its buddy to see if they are both free. 
   This recursive coalescing process continues up the tree, either 
   restoring the entire free space or stopping when a buddy is found to 
   be in use

*** Other ideas ****
Issues with the top solutions are scaling. Specifically, searching lists 
   can be quite slow. Thus, advanced allocators use more complex data 
   structures to address these costs, trading simplicity for 
   performance. Examples include balanced binary trees, splay trees, or 
   partially-ordered trees




Chapter 18: Pageing intro
paging: to chop up space into fixed-sized pieces. Instead of splitting 
   up a process’s address space into some number of variable-sized 
   logical segments (e.g., code, heap, stack), we divide it into 
   fixed-sized units, each of which we call a page. 
   We view physical memory as an array of fixed sized slots called page 
   frames, each of these frames can contain a single virtual memory page 

***A Simple Example and Overview***
advantage is the simplicity of free-space management that paging 
   affords. For example, when the OS wishes to place our tiny 64-byte
   address space into our eight-page physical memory, it simply finds 
   four free pages; perhaps the OS keeps a free list of all free pages 
   for this, and just grabs the first four free pages off of this list.

To record where each virtual page of the address space is placed in
   physical memory, the operating system usually keeps a per-process data
   structure known as a page table.
   The major role of the page table is to
   store address translations for each of the virtual pages of the 
   address space, thus letting us know where in physical memory each 
   page resides.

When a process generates a virtual address, the OS and hardware
   must combine to translate it into a meaningful physical address. 
   Turning “21” into binary form, we get “010101”, Thus, the virtual 
   address “21” is on the 5th (“0101”th) byte of virtual
   page “01” (or 1). physical frame number (PFN) (also sometimes
   called the physical page number or PPN) is 7 (binary 111). Thus, we 
   can translate this virtual address by replacing the VPN with the PFN 
   and then issue the load to physical memory (pg4 figure 18.3) Our 
   final physical address is 1110101 (117 in decimal), and is exactly 
   where we want our load to fetch data from (Figure 18.2, page 2).

***Where are pages stored***
we store the page table for each process in memory somewhere.
   much of OS memory itself can be virtualized, and thus page tables can 
   be stored in OS virtual memory (andeven swapped to disk),

Pages can get huge, a 32bit address with 4k pages can be a 20-bit vpn 
   and 12 bit offset.(2^20)

page table. In general, a page table stores virtual-to-physical address 
   translations, thus letting the system know where each page of an 
   address space actually resides in physical memory. Because each 
   address space requires such translations, in general there is one 
   page table per process in the system.

***Whats in the page table***
The page table is just a data structure that is used to map virtual 
   addresses (or really, virtual page numbers) to physical addresses 
   (physical frame numbers).

Linear page table is an array. The OS indexes the array by the virtual 
   page number and looks up the page-table entry(PTE) in order to find 
   the desired physical frame number(PFN) 
   Bits:
      Valid Bit: common to indicate whether the particular translation 
         is valid for example, when a program starts running, it will 
         have code and heap at one end of its address space, and the 
         stack at the other
      Invalid Bit: All the unused space in-between will be marked as 
         invalid and if the process tries to access the invalid memory 
         it will generate a trap to the OS which will likely terminate 
         the process.
      Protection Bit: indicating whether the page could be read from, 
         written to, or executed from. Accessing these bits will throw
         a trap
      Present Bit: indicate where the page is in physical memory or 
         disk (if if was swapped out)
      Dirty Bit: incidating whether the page was modified since it was 
         brought into memory 
      Reference Bit(accessed bit): sometimes used to track whether a 
         page has been accessed, useful to determine which pages are 
         popular and should be kept in memory 
Page table entry from the x86 architecture [I09]. It contains a 
   present bit (P); a read/write bit (R/W) which
   determines if writes are allowed to this page; a user/supervisor bit
   (U/S) which determines if user-mode processes can access the page; a 
   few bits (PWT, PCD, PAT, and G) that determine how hardware caching 
   works for these pages; an accessed bit (A) and a dirty bit (D); and 
   finally, the page frame number (PFN) itself.
Once this physical address is known, the hardware can fetch the PTE
   from memory, extract the PFN, and concatenate it with the offset from 
   the virtual address to form the desired physical address. Finally, 
   the hardware can fetch the desired data from memory and put
   it into register eax.
And now you can hopefully see that there are two real problems that
   we must solve. Without careful design of both hardware and software,
   page tables will cause the system to run too slowly, as well as take 
   up too much memory

***Memory trace***
When a program runs each instruction fetch will generate two memory 
   references: one to the page table to find the physical frame that the 
   instruction resides within, and one to the instruction itself to 
   fetch it to the CPU for processing.
In addition, there is one explicit memory reference in the form of
   the mov instruction; this adds another page table access first (to 
   translate the array virtual address to the correct physical one) and 
   then the array access itself.

***Paging Advantages 
First, it does not lead to external
   fragmentation, as paging (by design) divides memory into fixed-sized
   units. Second, it is quite flexible, enabling the sparse use of 
   virtual address spaces.
***Disadvantages
implementing paging support without care will lead to a
   slower machine (with many extra memory accesses to access the page
   table) as well as memory waste (with memory filled with page tables 
   instead of useful application data).

Chapter 19: Paging - Faster Translations (TBLs)
Paging requires a large amount of mapping information. mapping 
   information is generally stored in physical memory, paging logically 
   requires an extra memory lookup for each virtual address generated by 
   the program.

To speedaddress translation, we are going to add what is called (for 
   historical reasons [CP78]) a translation-lookaside buffer, or TLB 
   [CG68, C95]. A TLB is part of the chip’s memory-management unit (MMU)
   , and is  simply a hardware cache of popular virtual-to-physical 
   address translations; thus, a better name would be an 
   address-translation cache.

Upon each virtual memory reference, the hardware first checks the TLB to 
   see if the desired translation is held therein; if so, the 
   translation is performed (quickly) without having to consult the page 
   table (which has all translations).

***TLB Basic Algorithum*** 
The algorithm the hardware follows works like this: first, extract the
   virtual page number (VPN) from the virtual address 
   check if the TLB holds the translation for this VPN, if it does we get
   a TLB hit, which means the TLB holds the translation. Now we can 
   extract the page frame number from the relevant TLB entry, 
   concatenate that onto the offset from the original virtual address 
   and desired physical address and access memory assuming protection 
   checks do not fail 

If the CPU does not find the translation in the TLB (a TLB miss), we
   have some more work to do. the hardware accesses the

Page table to find the translation (Lines 11–12), and, assuming that the
   virtual memory reference generated by the process is valid and 
   accessible (Lines 13, 15), updates the TLB with the translation (Line 
   18). Theseset of actions are costly, primarily because of the extra 
   memory reference needed to access the page table. Once the TLB is 
   updated the hardware retries the instruction; this time the 
   transaltion is found in the TLB and the memory reference is processes 
   quickly 

***Accessing an Array***
Let us summarize TLB activity during our ten accesses to the array:
   miss, hit, hit, miss, hit, hit, hit, miss, hit, hit. Thus, our TLB hit rate, which is the number of hits divided by the total number of 
   accesses, is 70%.

TLB improves performance due to spatial locality. The elements of the 
   array are packedtightly into pages (i.e., they are close to one 
   another in space), and thus only the first access to an element on a 
   page yields a TLB miss. If the array had simply been twice as big (32 
   bytes, not 16), the array access would suffer even fewer misses.

One last point about TLB performance: if the program, soon after this
   loop completes, accesses the array again, we’d likely see an even 
   better result, assuming that we have a big enough TLB to cache the 
   needed translations:

***Who Handles the TLB Miss***
modern architectures (e.g., MIPS R10k [H93] or Sun’s SPARC v9
   [WG00], both RISC or reduced-instruction set computers) have what is
   known as a software-managed TLB. On a TLB miss, the hardware simply 
   raises an exception (line 11 in Figure 19.3), which pauses the current
   instruction stream, raises the privilege level to kernel mode, and 
   jumps to a trap handler. As you might guess, this trap handler is 
   code within the OS that is written with the express purpose of 
   handling TLB misses.

When run, the code will lookup the translation in the page table, use 
   special “privileged” instructions to update the TLB, and return from 
   the trap; at this point, the hardware retries the instruction 
   (resulting in a TLB hit).

First, the return-from-trap instruction needs to be a little different  
   than the return-from-trap we saw before when servicing a system call.

In the latter case, the return-fromtrap should resume execution at the 
   instruction after the trap into the OS, just as a return from a 
   procedure call returns to the instruction immediately following the 
   call into the procedure. In the former case, when returning from a 
   TLB miss-handling trap, the hardware must resume execution at the 
   instruction that caused the trap; this retry thus lets the 
   instruction run again, this time resulting in a TLB hit.

Second, when running the TLB miss-handling code, the OS needs to be
   extra careful not to cause an infinite chain of TLB misses to occur
   example, you cna keep TLB miss handlers in physical memory where they 
   are unmapped and not sugbject to address translation or keep them in 
   the TLB perm-valid translations and use some of those perm slots for 
   the handler itself, these are called wired translations and always 
   hit TLB

The primary advantage of the software-managed approach is 
   flexibility: the OS can use any data structure it wants to implement 
   the page table, without necessitating hardware change. 
Another advantage is simplicity, as seen in the TLB control flow
The hardware doesn’t do much on a miss:just raise an exception and let 
   the OS TLB miss handler do the rest.

***What's in there*** 
Basically, this just means that any given translation can be anywhere
   in the TLB, and that the hardware will search the entire TLB in 
   parallel to find the desired translation. 

A typical TLB might have 32, 64, or 128 entries and be what is called 
   fully associative cache 

***Context Switches***
when switching from one process to another, the hardware or OS (or both)
   mustbe careful to ensure that the about-to-be-run process does not 
   accidentally use translations from some previously run process

One approach is to simply flush the TLB on context switches, thus 
   emptying it before running the next process. On a software-based 
   system, thiscan be accomplished with an explicit (and privileged) 
   hardware instruction; with a hardware-managed TLB, the flush could be 
   enacted when the page-table base register is changed. The flush sets 
   all valid bits to 0
      Cost: there is a cost: each time a process runs, it
      must incur TLB misses as it touches its data and code pages.
      To reduce this overhead, some systems add hardware support to 
      enable sharing of the TLB across context switches. Hardware 
      provides an address space identifier 

you may also have thought of another case where two
   entries of the TLB are remarkably similar.

***Replacement Policy***
When replacing a cache how do you decide which one to replace
   One common approach is to evict the least-recently-used or LRU entry.
      LRU tries to take advantage of locality in the memory-reference 
      stream, assuming it is likely that an entry that has not recently 
      been used is a good candidate for eviction. 

   Another typical approach is to use a random policy, which evicts a 
      TLB mapping at random. Such a policy is useful due
      to its simplicity and ability to avoid corner-case behaviors;
   
Summary:
We have seen how hardware can help us make address translation
   faster. By providing a small, dedicated on-chip TLB as an   
   address-translation cache, most memory references will hopefully be 
   handled without having to access the page table in main memory. 

if the number of pages a program accesses in a short
period of time exceeds the number of pages that fit into the TLB, the 
program will generate a large number of TLB misses, and thus run quite a
bit more slowly. We refer to this phenomenon as exceeding the TLB 
coverage, and it can be quite a problem for certain programs
   One solution is include support for larger page
      sizes; by mapping key data structures into regions of the 
      program’s address space that are mapped by larger pages, the 
      effective coverage ofthe TLB can be increased. Support for large 
      pages is often exploited by programs such as a database management 
      system (a DBMS), which have certain data structures that are both 
      large and randomly-accessed.

TLB access can easily become a bottleneck in the CPU pipeline, in 
   particular with what is called a physically-indexed cache. With such 
   a cache, address translation has to
   take place before the cache is accessed, which can slow things down 
   quite a bit.

Chapter 20: Smaller Tables 
We now tackle the second problem that paging introduces: page tables
   are too big and thus consume too much memory

We could reduce the size of the page table in one simple way: use
   bigger pages

internal fragmentation(as the waste is internal to the unit of 
   allocation). Applications thus end up allocating pages but only using 
   little bits and pieces of each, and memory quickly fills up with 
   these overly-large pages.
      most systems use relatively small page sizes in the common case:
   
***Hybrid Approach: Paging and Segments***
Dennis had the idea of combining paging and segmentation in order to 
   reduce the memory overhead of page tables.

hybrid approach: instead of having a single page table for
   the entire address space of the process, why not have one per logical 
   segment? In this example, we might thus have three page tables, one 
   for the code, heap, and stack parts of the address space

segmentation, we had a base register that told
   us where each segment lived in physical memory, and a bound or limit
   register that told us the size of said segment. In our hybrid, we 
   still have those structures in the MMU; here, we use the base not to 
   point to the segment itself but rather to hold the physical address 
   of the page table of that segment.

The critical difference in our hybrid scheme is the presence of a bounds
   register per segment; each bounds register holds the value of the 
   maximum valid page in the segment.

Problems with hybrid:
   First, it still requires us to use segmentation; as we discussed 
      before, segmentation is not quite as flexible as we would like, as 
      it assumes a certain usage pattern of the address space; if we 
      have a large but sparsely-used heap, for example, we can still end 
      up with a lot of page table waste.
   Second, this hybrid causes external fragmentation to arise again. 
      While most of memory is managed in page-sized units, page tables 
      now can be of arbitrary size (in multiples of PTEs). Thus, finding 
      free space for them in memory is more complicated.

***Multi-level Page Tables***
how to get rid of all those invalid regions in the page table instead of 
   keeping them all in memory? We call this approach a multi-level
   page table, as it turns the linear page table into something like a 
   tree.
***Modern Systems employ this method - the method above***
The idea of the multi-level page steps are; first chop up the page table 
   into page-size units, then if valid, where it
   is in memory), use a new structure, called the page directory. The 
   page directory thus either can be used to tell you where a page of 
   the page table is, or that the entire page of the page table contains 
   no valid pages.

The page directory, in a simple two-level table, contains one entry per
   page of the page table. 
It consists of a number of page directory entries (PDE). A PDE 
   (minimally) has a valid bit and a page frame number (PFN), similar to 
   a PTE. 

***Advantages
First, and perhaps most obviously, the multi-level table only allocates 
   page-table space in proportion to the amount of address
   space you are using; thus it is generally compact and supports sparse 
   address spaces.

Second, if carefully constructed, each portion of the page table fits
   neatly within a page, making it easier to manage memory; the OS can
   simply grab the next free page when it needs to allocate or grow a 
   page table

***Cost
on a TLB miss, two loads from memory will be required to get the right 
   translation information from the page table (one for the page 
   directory, and one for the PTE itself), in contrast to just one load 
   with a linear page table
Whether it is the hardware or OS handling the page-table lookup (on a 
   TLB miss), doing so is undoubtedly more involved than a simple linear 
   page-table lookup.

***A Detailed Multi-Level Example***
page-directory index (PDIndex for short) from
   the VPN, we can use it to find the address of the page-directory entry
   (PDE) with a simple calculation:
   PDEAddr = PageDirBase + (PDIndex * sizeof(PDE))

If the page-directory entry is marked invalid, we know that the access
   is invalid, and thus raise an exception. If, however, the PDE is 
   valid, we have more work to do.

Specifically, we now have to fetch the pagetable entry (PTE) from the 
   page of the page table pointed to by this pagedirectory entry. To 
   find this PTE, we have to index into the portion of the
   page table using the remaining bits of the VPN:

page-table index (PTIndex for short) can then be used to index
   into the page table itself, giving us the address of our PTE:
   PTEAddr = (PDE.PFN << SHIFT) + (PTIndex * sizeof(PTE))

Note that the page-frame number (PFN) obtained from the page-directory
   entry must be left-shifted into place before combining it with the 
   pagetable index to form the address of the PTE

***More than 2 levels***
To determine how many levels are needed in a multi-level table to
   make all pieces of the page table fit within a page, we start by 
   determining how many page-table entries fit within a page
If our page directory has 2 14 entries, it spans not one page but 128, 
   and thus our goal of making every
   piece of the multi-level page table fit into a page vanishes.
To remedy this problem, we build a further level of the tree, by 
   splitting the page directory itself into multiple pages, and then 
   adding another page directory on top of that, to point to the pages 
   of the page directory

Now, when indexing the upper-level page directory, we use the very
top bits of the virtual address (PD Index 0 in the diagram); this index
can be used to fetch the page-directory entry from the top-level page 
directory. 
   If valid, the second level of the page directory is consulted by 
   combining the physical frame number from the top-level PDE and the 
   next part of the VPN (PD Index 1).
   Finally, if valid, the PTE address
   can be formed by using the page-table index combined with the address
   from the second-level PDE.

***The Translation Process: Remember the TLB***
before any of the complicated multilevel page table access occurs, the 
   hardware first checks the TLB; upon
   a hit, the physical address is formed directly without accessing the 
   page table at all, as before.
Only upon a TLB miss does the hardware need to
   perform the full multi-level lookup. On this path, you can see the ]cost of our traditional two-level page table:
   cost:
       two additional memory accesses to look up a valid translation.

***Inverted Page Tables***
instead of having many page tables (one per process of the system), we 
   keep a single page table that has an entry for each physical page of 
   the systemA linear scan would be expensive, and thus a hash table
   is often built over the base structure to speed up lookups.

***Swapping the Page Tables to Disk***
Thus, some systems place such page tables in kernel virtual memory,
   thereby allowing the system to swap some of these page tables to disk
   when memory pressure gets a little tight. 

***Summary***
The trade-offs such tables present are in time and space — the bigger
   the table, the faster a TLB miss can be serviced, as well as the 
   converse — and thus the right choice of structure depends strongly on 
   the constraints of the given environment.


Chapter 21: Beyond Physical Memory: Mechasnisms 
to support large address spaces, the OS will need a place to
   stash away portions of address spaces that currently aren’t in great 
   demand. 
   In general, the characteristics of such a location are that 
   it should have more capacity than memory; as a result, it is 
   generally slower (if it were faster, we would just use it as memory, 
   no?). 
   In modern systems, this role is usually served by a hard disk 
   drive.

***Swap Space***
he first thing we will need to do is to reserve some space on the disk
for moving pages back and forth. In operating systems, we generally refer
to such space as swap space, because we swap pages out of memory to it
and swap pages into memory from it.
   We assume that OS can read and write to the swap space, in page-sized
   units. The OS needs to remember the disk address of a given page 

We should note that swap space is not the only on-disk location for
   swapping traffic

However, if the system needs to make room in physical memory for other 
   needs, it can safely re-use the memory space for these code pages, 
   knowing that it can later swap them in again from the on-disk binary 
   in the file system.

***Present Bit***
The running process generates virtual memory references (for instruction 
   fetches, or data accesses), and, in this case, the hardware 
   translates them into physical addresses before fetching the desired 
   data from memory.

If we want to allow pages to be swapped we need to add more machinary; 
   the hardware looks in the PTE, it may find that the page is not 
   present in physical memory. The way the hardware (or the OS, in a 
   software-managed TLB approach) determines this is through a new piece 
   of information in each page-table entry, known as the present bit. 
      If the present bit is set to one, it means the
      page is present in physical memory and everything proceeds as 
      above; if it is set to zero, the page is not in memory but rather 
      on disk somewhere.
         The act of accessing a page that is not in physical memory is commonly referred to as a page fault.

***The Page fault***
TLB misses, we have two types of systems: hardwaremanaged TLBs (where 
   the hardware looks in the page table to find the
   desired translation) and software-managed TLBs (where the OS does).
      In both systems: if a page is not present, the OS is put in charge to handle the page fault. The appropriately-named OS page-fault handler runs to determine what to do. 

If a page is not present and has been swapped to disk, the OS will need
   to swap the page into memory in order to service the page fault.

When the OS receives a page fault for a page, it looks in the PTE to 
   find the address, and issues the request to disk to fetch the page 
   into memory.

***What if memory full?***
Thus, the OS might like to first page out one or more pages to make room
   for the new page(s) the OS is about to bring in. The process of 
   picking a page to kick out, or replace is known as the 
   page-replacement policy.

***Page Fault Control Flow***
three important cases to understand when a TLB miss occurs. 
First, that the page was both present and valid (Lines 18–21); 
   in this case, the TLB miss handler can simply grab the PFN from the 
   PTE, retry the instruction (this time resulting in a TLB hit), and 
   thus continue as described (many times) before.
In the second case (Lines 22–23), the page fault handler must be run, 
   although this was a legitimate page for the process to access (it is 
   valid, after all), it is not present in physical memory.
Third (and finally), the access could be to an invalid page, 
   due for example to a bug in the program (Lines 13–14). In this case, 
   no other bits in the PTE really matter; the hardware traps this 
   invalid access, and the OS trap handler runs, likely terminating the 
   offending process.

to service the page fault. First, the OS must find a physical frame for 
   the soon-to-be-faulted-in page to reside within; if there is no such 
   page, we’ll have to wait for the replacement algorithm to run and 
   kick some pages out of memory, thus freeing them for use here.
      With a physical frame in hand, the handler then issues the I/O 
      request to read in the page from swap space. Finally, when that 
      slow operation completes, the OS updates the page table and 
      retries the instruction. The retry will result in a TLB miss, and 
      then, upon another retry, a TLB hit,

***When Replacements Really Occur***
To keep a small amount of memory free, most operating systems thus
   have some kind of high watermark (HW) and low watermark (LW) to
   help decide when to start evicting pages from memory. 
      when the OS notices that there are fewer than LW pages available, 
         a background thread that is responsible for freeing memory runs.
         The thread evicts pages until there are HW pages available. The 
         background thread, sometimes called the swap daemon or page 
         daemon then goes to sleep, happy that it has freed some memory 
         for running processes and the OS to use.

To work with the background paging thread, the control flow in figure 
   21.3 should be modified by performing a replacement
   directly, the algorithm would instead simply check if there are any 
   free pages available. If not, it would inform the background paging 
   thread that free pages a

***Summary***
When the present bit is 0, the operating system page-fault handler runs 
   to service the page fault, and thus arranges for the transfer of the 
   desired page from disk to memory, perhaps first replacing some pages 
   in memory to make room for those soon to be swapped in.

This is all done transparently so the process believes that its         
   accessing its down private memory


Chapter 22: Beyond Physical Memory - Policies 
In such a case, this memory pressure forces the OS to start paging
   out pages to make room for actively-used pages. Deciding which page
   (or pages) to evict is encapsulated within the replacement policy of 
   the OS;

***Cache Management***
our goal in picking a replacement
   policy for this cache is to minimize the number of cache misses, i.e.
   to minimize the number of times that we have to fetch a page from 
   disk. Alternately, one can view our goal as maximizing the number of 
   cache hits, i.e., the number of times a page that is accessed is 
   found in memory.

Knowing the number of cache hits and misses let us calculate the average 
   memory access time (AMAT) for a program
   AMAT = TM + (PMiss · TD) 
   TM represents the cost of accessing memory, TD the cost of accessing 
   disk, and PMiss the probability of not finding the data in the
   cache (a miss)
      PMiss varies from 0.0 to 1.0, and sometimes we refer to
      a percent miss rate instead of a probability

***Optimal Replacement Policy***
The optimal replacement policy leads to the fewest number of misses 
   overall. Belady showed that a simple (but, unfortunately, difficult 
   to implement!) approach that replaces the page that will be accessed 
   furthest in the future is the optimal policy, resulting in the 
   fewest-possible cache misses.
      if you have to throw out some page, why not throw
      out the one that is needed the furthest from now? By doing so, you 
      are essentially saying that all the other pages in the cache are 
      more important than the one furthest out. The reason this is true 
      is simple: you will refer to the other pages before you refer to 
      the one furthest out

a miss is sometimes referred to as a cold-start miss (or compulsory miss)

We can also calculate the hit rate for the cache: with 6 hits and 5 
   misses, the hit rate is 
   Hits / (Hits+Misses) which is 6 / (6+5) or 54.5%

The optimal policy will thus serve only as a comparison point, to know 
   how close we are to “perfect”.

***A Simple Policy: FIFO***
FIFO (first-in, first-out) replacement, where pages
   were simply placed in a queue when they enter the system; when a 
   replacement occurs, the page on the tail of the queue (the “first-in” 
   page) is evicted. 
      FIFO has one great strength: it is quite simple to implement.
      Comparing FIFO to optimal, FIFO does notably worse: a 36.4% hit
      rate (or 57.1% excluding compulsory misses). FIFO simply can’t 
      determine the importance of blocks:

***Another Simple Policy: Random***
replacement policy is Random, which simply picks a
   random page to replace under memory pressure.
      Random has properties similar to FIFO; it is simple to implement, 
      but it doesn’t really try to be too intelligent in picking which 
      blocks to evict.
   Problem: it might kick out an important page, one that
   is about to be referenced again.

How to prevent removing an important page:
   frequency; if a page has been accessed many times, perhaps it
   should not be replaced as it clearly has some value. 
   recency of access; the more recently a page
   has been accessed, perhaps the more likely it will be accessed again.

This family of policies is based on what people refer to as the 
   principle of locality
   basically is just an observation about programs and their behavior. 
   What this principle says, quite simply, is that
   programs tend to access certain code sequences (e.g., in a loop) and 
   data structures (e.g., an array accessed by the loop) quite 
   frequently; we should thus try to use history to figure out which 
   pages are important, and keep those pages in memory when it comes to 
   eviction time.

Least-Frequently-Used (LFU) policy replaces the least-frequentlyused 
   page when an eviction must take place.
Least-RecentlyUsed (LRU) policy replaces the least-recently-used page.

MostFrequently-Used (MFU) and Most-Recently-Used (MRU). In most cases
   (not all!), these policies do not work well, as they ignore the 
   locality most programs exhibit instead of embracing it

***Workload Example***
First, when there is no locality in the workload, it doesn’t matter much 
   which realistic policy you are using; LRU, FIFO, and Random all 
   perform the same, with the hit rate exactly determined by the size of 
   the cache. 
Second, when the cache is large enough to fit the entire workload, it   
   also doesn’t matter which policy you use; all policies (even Random) 
   converge to a 100% hit rate when all the referenced blocks fit in 
   cache.
Finally, you can see that optimal performs noticeably better than the 
   realistic policies; peeking into the future, if it were possible, 
   does a much better job of replacement.

***Implementing Historical Algorithms***
LRU. To implement it perfectly, we need to
   do a lot of work. Specifically, upon each page access (i.e., each 
   memory access, whether an instruction fetch or a load or store), we 
   must update some data structure to move this page to the front of the 
   list (i.e., the MRU side).

FIFO, where the FIFO list of pages is only
   accessed when a page is evicted (by removing the first-in page) or 
   when a new page is added to the list (to the last-in side)

Unfortunately, as the number of pages in a system grows, scanning a
   huge array of times just to find the absolute least-recently-used 
   page is prohibitively expensive

***Approximating LRU***
